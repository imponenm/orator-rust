I'm getting a matmul error in the decoder step now: Error during inference: shape mismatch in matmul, lhs: [1, 16, 1, 19], rhs: [1, 16, 1, 64]

I'm pretty sure this has something to do with how the cache is created and be ing forwarded

Ther emight be a prblem with order. Each cache should correlate to an Attention struct




Decoder - Oirignal Implementation:
-----------------------------------
Lowest abstraction is Attention, which has the root type for kv_cache of Option<(Tensor, Tensor)>
Attention --> self.kv_cache --> Option<(Tensor, Tensor)>

Each layer has two Attention objects containing a kv_cache. So really, we need to pass the full Attention objects, not just the kv_caches
DecoderLayer --> self.self_attn & self.encoder_attn --> Attention

When initialized, create vec of length cfg.num_hidden_layers, where each value is a new DecoderLayer
Decoder --> self.layers --> Vec<DecoderLayer>

If I could create this Decoder object outside of Model, I think I'd be golden. And I could do this by passing the decoder object to both the Model's initializer and forward() function.
Model --> self.decoder --> Decoder


Text Encoder - Original Implementation:
---------------------------------------
Model --> self.text_encoder --> T5EncoderModel
T5EncoderModel --> self.T5Stack --> ...

T5EncoderModel has a T5Stack that eventually boils down to a T5Attention that has a kv_cache

So just like how we pulled the Decoder out of Model and passed it to generate(), we can also pull T5EncoderModel out of Model and pass it to generate, then call text_encoder.forward() instead of self.text_encoder.forward()