I'm getting a matmul error in the decoder step now: Error during inference: shape mismatch in matmul, lhs: [1, 16, 1, 19], rhs: [1, 16, 1, 64]

I'm pretty sure this has something to do with how the cache is created and be ing forwarded

Ther emight be a prblem with order. Each cache should correlate to an Attention struct




Oirignal Implementation:
-------------------------
Lowest abstraction is Attention, which has the root type for kv_cache of Option<(Tensor, Tensor)>
Attention --> self.kv_cache --> Option<(Tensor, Tensor)>

Each layer has two Attention objects containing a kv_cache. So really, we need to pass the full Attention objects, not just the kv_caches
DecoderLayer --> self.self_attn & self.encoder_attn --> Attention

When initialized, create vec of length cfg.num_hidden_layers, where each value is a new DecoderLayer
Decoder --> self.layers --> Vec<DecoderLayer>

If I could create this Decoder object outside of Model, I think I'd be golden. And I could do this by passing the decoder object to both the Model's initializer and forward() function.
Model --> self.decoder --> Decoder

