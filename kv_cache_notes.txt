TODO:
----------
Alright, i think I figure it out

Using the mini model was giving me issues. The config files are different in each repo, and it makes me think one of them isn't being loaded properly

So for now I can just use the large model

Next steps are reverting to most recent branch, and trying again on AWS. Maybe copy the main-cpy.rs file just because i added debug printing to the Args param. That's what helbed me troubleshoot






Decoder - Oirignal Implementation:
-----------------------------------
Lowest abstraction is Attention, which has the root type for kv_cache of Option<(Tensor, Tensor)>
Attention --> self.kv_cache --> Option<(Tensor, Tensor)>

Each layer has two Attention objects containing a kv_cache. So really, we need to pass the full Attention objects, not just the kv_caches
DecoderLayer --> self.self_attn & self.encoder_attn --> Attention

When initialized, create vec of length cfg.num_hidden_layers, where each value is a new DecoderLayer
Decoder --> self.layers --> Vec<DecoderLayer>

If I could create this Decoder object outside of Model, I think I'd be golden. And I could do this by passing the decoder object to both the Model's initializer and forward() function.
Model --> self.decoder --> Decoder


Text Encoder - Original Implementation:
---------------------------------------
Model --> self.text_encoder --> T5EncoderModel
T5EncoderModel --> self.T5Stack --> ...

T5EncoderModel has a T5Stack that eventually boils down to a T5Attention that has a kv_cache

So just like how we pulled the Decoder out of Model and passed it to generate(), we can also pull T5EncoderModel out of Model and pass it to generate, then call text_encoder.forward() instead of self.text_encoder.forward()